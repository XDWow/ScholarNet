## ScholarNet

### 简介

​	这是一个学术交流平台，面向学术交流场景，支持用户通过文章、评论、互动、打赏、标签和搜索等功能进行交流，对事件的存储，分发，治理，容错，安全，可以理解成一个大型的论坛项目，虽然项目创新点不足，但在应用上可学习相关内容，项目采用微服务架构，并且对高并发，高可用，高性能进行全方位思考和实践，并且接入日志监控和链路追踪，此项目只供个人学习相关，前端部分相对简陋，以下设计针对后端进行实现，根据提交内容可以查看对应的进度，为每个开发阶段制定测试方案，包括单元测试、集成测试和负载测试，确保系统稳定可靠，当前仍在建设中。

### 技术栈：Gin + Gorm + MySQL + Redis + Kafka + etcd + ElasticSearch + gRPC

### 开发环境

IDE🧑‍💻： [GoLand](https://www.jetbrains.com/go/)

go 1.22.0

OS🪟🐧：[Ubuntu 22.04.3 LTS (WSL2)](https://ubuntu.com/desktop/wsl)

### 业务分析

#### BFF服务

长短token设计，长token的7天过期，普通token的30分钟过期，并且redis只存储退出登录用户的jwtToken，使用的时候把它存入ctx中的头部header，x-jwt-token，x-refresh-token 以及对应的user 可以获取到UserClaims，这块东西做成了一个middleware去进行，

session检查机制，会根据ssid，并且存储在cookie ，去判断是否当前用户以及下线，这里和jwt的二选一

user-agent登录校验user，更好的实践应该是浏览器指纹那类的

装饰器模式，限流的分发，用随机数去进行限流，判断是否进行远程调用，或者本地调用，这里拓展的是不同实例之间的调用

支持微信登录，规定好codeurl和callback，state把他存在到cookie里面，这里也做jwt校验

#### 用户服务

安全:
1.密码不采用明文存储，用md5加密后存入数据库

2.访问资源采用jwt，并且这个jwt其实可以做成一个middleware给gin使用，并且可以结合长短token一起使用

3.还需要请求头携带user-agent进行，更深的可以上升到浏览器指纹这块让前端去操作

4.注册和登录的逻辑采用insert for update，其实就是那句如果遇到id冲突就成更新
注册操作需要校验是否之前注册过，这里不要用先去查一次库的方式，而是直接插入由于手机号是独立的，去根据mysql的错误重复数据1042进行判断create是否成立的
这里面还有有个findById的操作，思路是这里采用查数据库后写进缓存，触发降级就不走数据库的快慢路径操作先走缓存，缓存找不到就从数据库找，找完重新set进去，如果redis出现错误， 就不让他流量直接打到数据库，因此直接返回错误

5.微信登录，先去url让前端弄成二维码，然后扫后等待微信回调一个方法等待接受，等待微信回调跳转，然后还有一个验证的流程，然后进行注册或者登录操作，主要是里面的unionid和openid

![](https://pic.imgdb.cn/item/674359dd88c538a9b5bb7534.png)

![](https://pic.imgdb.cn/item/67435abf88c538a9b5bb753f.png)

#### 短信服务

短信服务其实就是去套用各种模板，并且提取一个顶级抽象，有趣的点在于它的容错怎么处理，提供多种实现，同步转异步，权限验证，超时重试，限流，还有一些固定模板的实现

1.权限：传参数发送的时候，对应的模板id会被jwt加密，在这里需要类似装饰器模式进行一个解决后得到id

2.限流：设置限流后对短信服务进行拦截，也是结合redis和lua的通用

3.failover：这里首先采用的是数组开始遍历，但是发现流量都在第一个，就用求余法进行操作，还配置了最大超时次数就进行切换短信渠道商，这里切换的时候用的cas和一些原子类的操作，没有去加锁

4.同步转异步：

1.1 使用绝对阈值，比如说直接发送的时候，（连续一段时间，或者连续N个请求）响应时间超过了 500ms，然后后续请求转异步
1.2 或者send返回的错误率大于某个阈值
2.退出异步的方式就是连续请求和错误率都低于了阈值
实现的方式都是用的维护两个滑动窗口的方式，这个也可以做成动态可配置

对于发送的接口，先抢占，抢占到后发送，发送后记录数据给数据库，处理并发问题，结合数据库的for update锁，就是一个查完更新的操作，更新的同时计数加一并且只查一分钟前的记录，保证发送的时候不会有并发问题，因为时间已经被该实例更新，不会被其他拿到

#### 验证码服务

它只需要去调用短信服务，然后

在自己维护一个redis，结合lua脚本把输入和正确在里面确认，不要在代码会带来问题，来验证验证码是否正确，验证码的策略是十分钟有效期，并且超过一分钟就可以重新发送，在redis维护两个key就行，并且用户输入的验证码正确验证次数的标记-1，让他自己过期

然后还有本地缓存的实现方式，验证码这种东西没必要多级缓存，因此在创建的时候可以手动配置

#### 支付服务

这里用的是微信Native的支付方式，主要是讲一下思路和需要的api问题

思路：当业务方创建订单，这边传入对应的bizId和对应的数据，然后会在数据库初始化一条预支付信息，在web端暴露后接受微信方的回调后更新对应的时间，然后把数据发送到kakfa上，方便后面进行数据清洗（未实现），最重要的是给对应的业务方可以进行读取需要的内容，容错思路就是当业务方半小时没有返回，需要主动去微信那边查，这边开了个for去查数据库30分钟前的数据然后发送获取对应的结果，后更新数据。

api：这里用到了微信开源的github.com/wechatpay-apiv3/wechatpay-go可以查看对应的实现，还有需要初始化微信客户端的一些配置，这些放在ioc/wechat.go里面。*notify.Handler可以帮   在web端解析对应的请求&payments.Transaction{}
*native.NativeApiService提供一些主动查询的方法

![](https://pic.imgdb.cn/item/674727ccd0e0a243d4d2e8a4.png)



#### 打赏服务

流程： 打赏-支付-入账三个服务的调用

预打赏在打赏服务给的二维码这边做缓存，而不是在支付服务做的缓存，先查缓存，没有创建，调用支付服务，然后写缓存，查询根据biz和bizId和uid进行查询，防止被人恶意查数据

获取打赏结果只需要用户去关心是否打赏成功，查询订单可以走一个快慢路径，先查本地表维护的状态，否则就去调用支付服务查状态，然后自己更新，出现这种情况是比如 预支付出现二维码后但是 还没支付   就进行getreward

更新状态的时候后再去查订单状态，就是只对状态是支付成功的操作，这里模拟了平台抽成的操作，然后都发给入账服务进行处理

kafka的清洗功能，在支付的时候，会把结果发送给kafka，topic是payment_event 那么，在打赏服务这边起一个消费者reward-%d，只消费和自己有关数据，消费到的数据在进行更新到对应的数据库里面状态，可以做为一个二次校验的功能



![](https://pic.imgdb.cn/item/6747265ad0e0a243d4d2dfcd.png)

#### 入账服务

首先知道一个人可能会有多个账号，但是uid是固定的，因此 需要建立一张uid和account的表，以及一张入账记录的表，并且记录对应的业务id和对应金额。涉及的一些负数或者小数都可以通过字段解决，这里不做赘述实现，这里目前只维护了一个接口入账信息，并没有过多的审计，校验操作，插入数据的时候去也是开个事务的操作

#### 标签服务

场景是可以给资源打多个标签，并且标签支持搜索，个人也可以拥有多个标签，不支持级联标签，资源就是某人给某资源打了某些标签，因此要维护标签表和标签资源表，那么后者肯定会关联一个逻辑外键出来，这里处理方式是用gorm的关联外键，并且设置级联删除,并且打上标签的时候会开事务先删掉原有的标签，不支持修改标签.

引入缓存的机制，用户可以会有一个异步预加载的操作，对于tob最好可以多个查对应标签写到缓存策略

kafka的使用是把资源发送给es进行写入到inputAny里面，里面存的字段data和对应字符串

#### 搜索服务

搜索把他提取成了一个服务，这样就可以对接各种业务方带来的搜索功能，但是这里没有使用 multi query 或者 multi match 之类的写法，比如 搜索的内容有用户和文章，那么 就会去用俩个repository去搜查的操作，标签也能被搜索。以后需要对接不同业务方的时候需要自己去修改，当然这里也提供InputAny的方式，需要传入某人搜索了什么东西，传json数据也行，但是没有字段索引那么好找。

用kafka消费数据的时候就是读到业务方自己的topic的时候，就相当于去调用any，这也是一种兜底策略

#### 文章服务

文章这块，主要是会有一个三个状态，发表，未发表，自己可见，并且发表和未发表的会有两个数据表，走的思路也是结合redis进行一个缓存操作，基本都是回写缓存的操作。那么对于内存的折中，对于作者界面，网址缓存第一页，并且文章内容也是缓存的摘要，并且默认用户会点击第一篇文章进去看的次数比较多，如果查第一页的时候返回的是返回数据，那么 就异步进行预加载机制把第一篇文章给他全部缓存起来，并且内容不超过1MB限制，否则查库回写第一页数据。

在作者触发更新文章的时候需要把第一页缓存记得删除

还有一个思路是把文章内容缓存到s3或者mongodb里面，数据库只存那些数据

具有制作库和线上库的概念，不干扰线上生产环境，那么大部分发版的操作是需要进行事务的，对于插入操作其实就看id是否为0 ，而在线上库则需要事务控制提交和回滚，提供upsert语义

#### 交互服务

这里设计对应的点赞和收藏业务逻辑

表的设计，会有交互表，点赞表，收藏表，定义是否点赞和的收藏的逻辑，因此一些级联操作，然后也有很多双写入操作，采用的策略是事务，然后也定义了双写连接池自动commit的操作

缓存设计只缓存文章的点赞和收藏数量，采用结构是HMSet，并且阅读量用kafka进行异步批量消费削峰

数据迁移也在这里做，这里定义了的是同结构数据迁移，采用migrator进行数据迁移，结合canal对于数据的监听，然后发送到kafka里面进行异步消费

#### 热榜服务

这里定义了两个接口，一个是计算接口，一个是获取接口

对于热榜分数的计算采用点赞量和时间进行衰减，需要调用文章服务去分批获取所有的文章，然后根据每个文章去调用交互服务获取对应的点赞量，最后计算出这个文章相应的热榜分数，用一个优先队列来保存获取的热榜数据，借用了ekit包里面的NewPriorityQueue， 把大的分数都放在前面就解决，查找的时候也是根据utime排序后查找，不够一批的有对应的处理，并且防止并发问题，查表的时候会有一起始时间的限制

旁路模式双缓存设计，更新数据的时候先更新本地，再更新redis，获取的数据的时候，先从本地缓存获取，若有问题再获取redis缓存，再把redis得到的数据回写到本地缓存里面，如果redis出问题，就从本地缓存里面强制获取老的数据容错

#### 关注服务

表设计只有关注人和被关注着的表，对于粉丝量和关注量这种设计并不做持久化，只放在缓存，必要时做count查询

对于业务来说，当一个人进行关注，那么关注的数量就会增加一个，同时被关注者的粉丝也会增加，对于粉丝量和关注量，采用缓存里面去设计，并且用到reids的管道机制进行操作，由于查看自己的粉丝和关注是少量操作，并不缓存，如果需要查就会做一个count查询，然后缓存到redis里面，后面的增加操作就是直接操作缓存，当然这里设计还可以更好。

需要理清楚关注者和被关注者，用表查询解决，这里相对来说只是业务设计

#### 评论服务

评论的设计有三种方式，一种是父评论和子评论id树型设计，还有Nested Set进行嵌套设计，还有闭包类型的两张表设计，这里采用第一种设计模式，对应有根评论，父评论，子评论数组，相互嵌套的

设置级联删除，删除的时候会把父评论对应的给一起删了,可以理解成PID找不到的id的时候就会把自己也删掉

查找评论的时候并发去找三条子评论，并且存在降级策略，上游ctx可以控制是否触发降级，若触发则不再查询子评论，找子评论的方式究竟是循环并发每个子评论，并且注意闭包捕获的的问题

查询的时候增加minId和maxId的操作，防止并发出现的分页重复读取评论问题

通过是否id为0来判断是查询当前rootid主评论还是查找主评论和其所有子评论

#### 聊天服务

#### Feed流服务

#### 自定义扩展pkg

**logger**：这个主要是采用zapLogger拓展，在不适用其他日志现成框架的前提下，单纯用接口定义出对应常用的方法，并且采用Field的字段约束，并且用这个给gorm和gin进行拓展打印日志，在其他业务代码中也是采用这个

**saramax**:通过sarama的一些api进行二次封装，主要是对消费者去实现ConsumerGroupHandler就可以就消息的前后和处理的逻辑自己编写，并且引入泛型进行序列化读取操作，主要有批量消费和重试机制

**ginx**：

1.主要使用*gin.HandlerFunc 主要做的是对请求体的绑定到对应结构体和对claim进行校验的流程，也是采用泛型外面传进来，然后进行处理走对应的函数流程，完事后返回统一的result。

2.还有限流插件的操作，在一定时间段限制对应的客户端ip流量通过，主要是按照ip进行建key，然后走lua脚本用zset的数据结构，基本和其他限流操作差不多。

3.还有对请求和响应的打印日志，aop思想，扩展的插件可以显式配置是否需要打印对应的请求体和返回数据，难点在需要gin.ResponseWriter操作下面的http.ResponseWriter，对返回的数据进行操作。

4.此外还会统计活跃请求书和返回响应时间到Prometheus里面可选

**grpcx**：

1.采用自己设计的平滑的带权重的负载均衡，主要是要注意到balancer.Builder和sbalancer.Picker分别两个结构体去实现这两个接口，以etcd作为注册中心，并且加锁防止不同节点计算，balancer.SubConn作为节点存储，其中要注意注册中心转发数据后整会变浮点的

2.也对进行日志打印的操作，通过判断是否之前panic掉的数据来获取对应的err，然后打对应的一些日志，方法，响应时间，对端应用名和ip等。

**gormx**：

1.利用callback机制增加对sql查询的时间的Prometheus的操作

2.对更换数据库的时候双写方案，双写的一致性保证，使用方法在测试用例里面，拓展手动进行事务的提交和回滚也支持双数据源，主要利用的是gorm.SubConn和sql.Tx，后者可以更加精确的控制不同数据源的提交和回滚

**canalx、redis**：都是对一些操作的日志打印到Prometheus集成和一些通用东西的提取

**cronjobx**:对定时任务的统一管理，并且也加入链路追踪和日志打印的流程，主要还是去实现Job实现，这里把他做成了一个新的服务，自定义了任务的数据库板块，实现方式参考xxl-job，并且以grpc的方式暴露，不做服务注册

这里多一个cronjob的服务实现，对于任务采用抢占式调用，根据cron表达式设置下一调用时间字段，循环扫表找下一调用时间小于当前的时间的就进行抢占标记，并且更新使用时间，这个抢占过程采用的是CAS乐观锁去实现，查看是否影响数据

**ratelimit**：利用redis和lua脚本实现滑动窗口的限流操作

**migrator**：做的是同结构**不停机数据迁移**相关。**数据校验和数据修复**

1.首先不停机迁移的方案首先对于两个数据库会有四个状态，分别src，src首先，dst首先，dst，也就是读写和写的操作，这里需要结合之前的双写策略保证对于两个的数据源的数据一致性

2.数据校验，分为增量校验，增量需要注意offset的并发问题，可以使用utime或者canal进行解决，全量校验，在这个修复的策略用定时任务去处理，并且发送给kafka去异步进行修复，最大程度保证数据一致性。先去查一下对应的目的表，记录对应的列字段。

3.数据修复，并发问题需要考虑，在修复的同时，会有源库存在没有被同步到目的库，这里走select for upsert的语义就能解决，但是有个极端比如他同步的时候有源库删除数据，那么对于源库修复的时候也需要进行删除

4.支持http调用动态更改四个状态和对校验的启动和停止，具有高并发思考

5.解决offset和limit带来的掠过或者重复读取问题，方案是canal或者加utime

**架构**：
![image-20241119154948019](https://github.com/Hwoss-Pe/Webook/blob/main/image-20241119154948019.png)

### 技术栈

- [Node.js](https://nodejs.org/en)
- Docker
    - [镜像源](https://yeasy.gitbook.io/docker_practice/install/mirror)（还是挂代理方便）
    - [mysql](https://hub.docker.com/_/mysql) - An open-source relational database management system (RDBMS)
    - [redis](https://hub.docker.com/r/bitnami/redis) - An open-source in-memory storage
    - [etcd](https://hub.docker.com/r/bitnami/etcd) - A distributed key-value store designed to securely store data across a cluster
    - [mongo](https://hub.docker.com/_/mongo) - MongoDB document databases provide high availability and easy scalability
    - [kafka](https://hub.docker.com/r/bitnami/kafka) - Apache Kafka is a distributed streaming platform used for building real-time applications
    - [prometheus](https://hub.docker.com/r/bitnami/prometheus) - The Prometheus monitoring system and time series database
    - *grafana - The open observability platform*
    - *zipkin - A distributed tracing system*
    - Canal - Alibaba's open-source project to capture database changes (binlog) and synchronize to other systems. It is often used for data replication and real-time streaming.
    - ELK - A set of tools for logging, searching, and visualizing data:
        - Elasticsearch: A distributed, RESTful search and analytics engine
        - Logstash: A server-side data processing pipeline
        - Kibana: A visualization tool for Elasticsearch data
- kubernates
    - [Kubernetes cluster architecture](https://kubernetes.io/docs/concepts/architecture/)
    - [kubectl](https://kubernetes.io/docs/tasks/tools/) - The Kubernetes command-line tool
    - [HELM](https://helm.sh/) - The package manager for Kubernetes
    - [ingress-nignx](https://github.com/kubernetes/ingress-nginx) - Ingress-NGINX Controller for Kubernetes
- [wrk](https://github.com/wg/wrk) - Modern HTTP benchmarking tool
- [protobuf](https://github.com/protocolbuffers/protobuf) - Protocol Buffers - Google's data interchange format
- [buf](https://github.com/bufbuild/buf) - The best way of working with Protocol Buffers

### 编程能力

**分包架构**

这体现在对于各种服务之间的管理和应用，并且服务内遵循dao- repository-service-handler ，并且使用mock和wire对里面进行测试和依赖注入，并且采用DDD和TDD的一些实际运用，各个服务之间都采用rpc调用

**面向失败编程**

面向失败编程（Failure-oriented Programming，FOP）是一种编程范式，强调在编写逻辑时尽可能考虑边界条件和失败情况，以增强程序的稳定性。 简单来说，就是时刻考虑系统可能会崩溃。无论是系统本身、依赖的服务还是依赖的数据库，都可能会崩溃。 面向失败编程不仅仅是对输入进行校验，它还包括：

- 错误处理：需要严密处理各种可能的错误情况
- 容错设计：长期培养的能力是针对业务和系统特征设计容错策略。这通常是较难掌握的，而其余部分可以通过规范来达成。 在面向失败编程中，需要长期培养的能力是针对业务和系统特征设计容错策略。其他方面较容易掌握，或者公司可以通过规范来达成。 在项目中，讨论了许多容错方案，包括：
- 重试机制：需要考虑重试的间隔和次数，以及最后可能需要人工介入。
- 监控与告警：在追求高可用时，还要考虑自动修复的程度
- 限流：用于保护系统本身。
- 下游服务治理：如果下游服务可能崩溃，需使用一些治理技巧：
    - 轮询：可以是每次都轮询，也可以针对某个下游节点失败后的限流。
    - 客户端限流：限制客户端的请求速率以保护系统资源。
    - 同步转异步：在转为异步后，必须保证请求会被处理而不会遗漏
- 考虑安全性：例如，防止 token 泄露以增强系统的安全性。 在设计容错方案时，尽可能在平时收集别人使用的容错方案，以了解各种处理方式。根据自己实际处理的业务设计合适的容错方案。简单地生搬硬套别人的方案，效果可能不佳。

**灵活的缓存方案**

在整个单体应用中，已经充分接触了缓存方案。相比传统的缓存方案，项目中的缓存方案更具“趣味性”。在实践中，除非逼不得已，通常不会使用看起来非常特殊的缓存方案。 使用过和讨论过的缓存方案包括：

- 只使用 Redis：更新缓存的常见方案是更新数据库后删除缓存。
- 本地缓存与 Redis 缓存结合使用。大多数系统完成这些步骤即可，
    - 查找顺序：本地缓存 - Redis - 数据库
    - 更新顺序：数据库 - 本地缓存 - Redis
- 根据业务特征动态设置缓存的过期时间。例如，如果能判定某个用户是大 V，则他的数据过期时间应设得更长。
- 淘汰对象：根据业务特征来淘汰缓存对象。
- 缓存崩溃：需要考虑缓存崩溃的问题。在实践中，缓存崩溃可能导致数据库也一起崩溃。 在上述缓存方案的基础上，需要能够举一反三，根据业务特征设计针对性的解决方案。在整个职业生涯中，如果能有效使用缓存，就能解决 90% 的性能问题。剩下的 10% 则需要依靠各种技巧和优化手段。

**注意并发问题**

无论是代码中的 Go 实例，还是外部数据库，在实现任何功能时操作对象或 Redis 缓存数据时，都必须考虑并发问题。具体来说，需要关注是否有多个 goroutine 在同一时刻读写对象，这些 goroutine 可能在不同的实例（机器）上，也可能在同一实例（机器）上。 在项目中，使用了多种方法来解决并发问题：

- SELECT FOR UPDATE：用于确保读取的数据在操作期间不会被修改，简单且有效。
- 分布式锁：用于保证同一时刻只有一个 goroutine 可以执行特定操作。
- Lua 脚本：在 Redis 中使用 Lua 脚本来确保在执行多个操作时没有其他 goroutine 修改 Redis 数据。
- 乐观锁：使用数据库 version 加 CAS（Compare and Swap）机制来保证在修改数据时，数据未被其他操作修改过。
- Go 对象锁：使用 `sync.Mutex` 和 `sync.RWMutex` 来管理对 Go 对象的并发访问，在某些情况下，还可以使用原子操作（`atomic` 包）来处理简单的并发问题。 在实践中，只能通过长期训练来培养并发意识。在项目开始时，就应有意识地培养自己对并发问题的关注和敏感度。

**依赖注入**

首先要整体上领悟依赖注入和面向接口编程的优势，这些优点在项目中体现得非常明显：

- 依赖注入完全达成了控制反转的目标。不再关心如何创建依赖对象。例如，在 cache 模块中，虽然使用了 Redis 客户端，但 cache 实现并不关心具体的实现或客户端的相关参数。

- 依赖注入提高了代码的可测试性。可以在单元测试中注入由 `gomock` 生成的实例。在集成测试阶段，为了节省公司资源，第三方依赖通常被替换为内存实现或 mock 实现。

- 依赖注入叠加面向接口编程后，装饰器模式效果更佳。在 sms 模块中，有各种装饰器的实现，这些实现都是基于面向接口编程和依赖注入的。这使得装饰器可以自由组合，提升了系统的灵活性和扩展性。
